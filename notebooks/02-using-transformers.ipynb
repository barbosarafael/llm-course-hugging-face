{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Behind the pipeline \n",
    "\n",
    "1. preprocessing\n",
    "2. going through the model\n",
    "3. postprocessing\n",
    "\n",
    "### Preprocessing with a tokenizer\n",
    "\n",
    "- Transforming text into tokens (numbers)\n",
    "- Tokenization:\n",
    "  - Split the text in tokens (e.g. words, symbols)\n",
    "  - Each token has a corresponding id\n",
    "  - Add masks to indicate which tokens are missing\n",
    "- The same tokenizer have to be used in pre-training and inference\n",
    "- In this code `tokenizer = AutoTokenizer.from_pretrained(checkpoint)`\n",
    "  - We have `input_ids`: sequence of token's ID\n",
    "  - And `attention_mask`: mask who indicates which tokens are missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b354bd99f38441d97c3dffe4266cbbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fac449e931ef47d2a32f765288d5dcc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf5d0e57ec0a4c30be5a82edb27adbb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Going through the model\n",
    "\n",
    "<figure>\n",
    "  <img src=\"../images/transformer_and_head-dark.svg\" alt=\"Fluxo do pipeline\" width=\"500\"/>\n",
    "  <figcaption>Dispon√≠vel em: https://huggingface.co/learn/llm-course/chapter2/2</figcaption>\n",
    "</figure>\n",
    "\n",
    "- Tokens to internal representation\n",
    "- High dimensional vector: hidden states\n",
    "  - After pass the tensor to model, we get the hidden states\n",
    "  - With dimensions:\n",
    "    - Batch size\n",
    "    - Sequence length\n",
    "    - Hidden size\n",
    "  - For examploe: `torch.Size([2, 16, 768])`\n",
    "- Model heads: Transform hidden states to util things \n",
    "  - The principal model produces a high dimensional vector\n",
    "  - The head of the model get this vector and transform it in a specifical thing for the task (classification, generation, etc)\n",
    "  - Example of heads useds in Transformers Models\n",
    "    - ForCausalLM: predict the next token\n",
    "    - ForMaskedLM: predict the masked token\n",
    "    - ForSequenceClassification: predict the label\n",
    "    - ForTokenClassification: predict the label for each token\n",
    "    - ForQuestionAnswering: predict the answer\n",
    "  - In Sentiment Analysis we used AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model heads: Postprocessing the output\n",
    "\n",
    "- The output of the model is a logits, which are not a probability\n",
    "- We need to apply a softmax to get the probabilities\n",
    "\n",
    "```python\n",
    "predictions = softmax(output.logits, dim = -1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.5607,  1.6123],\n",
      "        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.0195e-02, 9.5981e-01],\n",
      "        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
